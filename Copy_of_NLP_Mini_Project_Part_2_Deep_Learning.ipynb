{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk keras gensim scikit-learn"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x__VGpdbRBS",
        "outputId": "9c0b1bb0-abf5-4f41-d580-072a8f84bd3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/NLP Manufacturer Dataset .csv')  # Replace with actual dataset path\n",
        "\n",
        "# Exploratory Data Analysis (EDA)\n",
        "print(\"Dataset overview:\")\n",
        "print(data.info())  # Checking for missing values, data types\n",
        "print(\"\\nSample data:\")\n",
        "print(data.head())  # Display the first few rows\n",
        "\n",
        "# Plot class distribution\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# data['Input'].value_counts().plot(kind='bar', color='skyblue')\n",
        "# plt.title('Class Distribution')\n",
        "# plt.xlabel('Class')\n",
        "# plt.ylabel('Count')\n",
        "# plt.show()\n",
        "\n",
        "# Splitting data into train, validation, and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q01lzLwtaIvu",
        "outputId": "82e5b1a7-9a22-45ad-b414-6d9a18b182be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset overview:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1549 entries, 0 to 1548\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Input       1549 non-null   object\n",
            " 1   Prediction  1547 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 24.3+ KB\n",
            "None\n",
            "\n",
            "Sample data:\n",
            "                                               Input    Prediction\n",
            "0  The production batch has been inspected thorou...     Compliant\n",
            "1  During the final inspection, some units were f...  Minor Defect\n",
            "2  A critical defect was identified in the batch ...   Major Issue\n",
            "3  All items in this batch passed the quality con...     Compliant\n",
            "4  There are some minor defects in a few units, s...  Minor Defect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the dataset\n",
        "file_name = \"/content/NLP Manufacturer Dataset .csv\"  # Ensure the file name matches\n",
        "df = pd.read_csv(file_name)\n",
        "\n",
        "# Check the first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Split the dataset into train, validation, and test sets (80% train, 10% val, 10% test)\n",
        "train_data, temp_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Function to clean and preprocess text\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):  # Ensure text is a string\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "        tokens = word_tokenize(text)  # Tokenize the text\n",
        "        tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
        "        return ' '.join(tokens)\n",
        "    return \"\"\n",
        "\n",
        "# Apply the preprocessing to the dataset\n",
        "train_data['processed_text'] = train_data['Input'].apply(preprocess_text)\n",
        "val_data['processed_text'] = val_data['Input'].apply(preprocess_text)\n",
        "test_data['processed_text'] = test_data['Input'].apply(preprocess_text)\n",
        "\n",
        "# Check the processed text\n",
        "print(train_data[['Input', 'processed_text']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqNvblU2bzaI",
        "outputId": "40db4190-7753-4a31-a50e-e2f470770244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Input    Prediction\n",
            "0  The production batch has been inspected thorou...     Compliant\n",
            "1  During the final inspection, some units were f...  Minor Defect\n",
            "2  A critical defect was identified in the batch ...   Major Issue\n",
            "3  All items in this batch passed the quality con...     Compliant\n",
            "4  There are some minor defects in a few units, s...  Minor Defect\n",
            "                                                  Input  \\\n",
            "1249  The product has been tested and meets all perf...   \n",
            "1048  Some wooden picture frames had slightly uneven...   \n",
            "1159  Some glass measuring cups had slightly faded v...   \n",
            "818   The product has a minor issue with the display...   \n",
            "741   A batch of medical face masks failed filtratio...   \n",
            "\n",
            "                                         processed_text  \n",
            "1249  product tested meets performance standards how...  \n",
            "1048  wooden picture frames slightly uneven edges du...  \n",
            "1159  glass measuring cups slightly faded volume mar...  \n",
            "818   product minor issue display screen remains fun...  \n",
            "741   batch medical face masks failed filtration eff...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nif1xXtxmnlr",
        "outputId": "afbf2dbf-d8b2-4b3e-d86f-5386483dba5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import gensim\n",
        "import numpy as np\n",
        "\n",
        "# Bag of Words (BoW)\n",
        "bow_vectorizer = CountVectorizer(max_features=5000)\n",
        "X_train_bow = bow_vectorizer.fit_transform(train_data['processed_text']).toarray() # train_data is now defined and accessible\n",
        "X_val_bow = bow_vectorizer.transform(val_data['processed_text']).toarray()\n",
        "X_test_bow = bow_vectorizer.transform(test_data['processed_text']).toarray()\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['processed_text']).toarray()\n",
        "X_val_tfidf = tfidf_vectorizer.transform(val_data['processed_text']).toarray()\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_data['processed_text']).toarray()\n",
        "\n",
        "# FastText (Word Embeddings)\n",
        "train_sentences = train_data['processed_text'].apply(str.split).tolist()\n",
        "val_sentences = val_data['processed_text'].apply(str.split).tolist()\n",
        "test_sentences = test_data['processed_text'].apply(str.split).tolist()\n",
        "\n",
        "# Train FastText model\n",
        "fasttext_model = gensim.models.FastText(sentences=train_sentences, vector_size=100, window=5, min_count=5)\n",
        "\n",
        "# Convert text to FastText embeddings\n",
        "def get_fasttext_vectors(sentences, model):\n",
        "    return np.array([np.mean([model.wv[word] for word in sentence if word in model.wv] or [np.zeros(100)], axis=0) for sentence in sentences])\n",
        "\n",
        "X_train_fasttext = get_fasttext_vectors(train_sentences, fasttext_model)\n",
        "X_val_fasttext = get_fasttext_vectors(val_sentences, fasttext_model)\n",
        "X_test_fasttext = get_fasttext_vectors(test_sentences, fasttext_model)\n",
        "\n",
        "print(\"Embedding completed for BoW, TF-IDF, and FastText.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI8iZN0_fCjJ",
        "outputId": "38294804-a98f-4d8f-a70e-ca51238c2999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding completed for BoW, TF-IDF, and FastText.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TpKeraQPgDtv",
        "outputId": "dccd2797-e4ba-416a-a179-601bee38801f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m594.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, google-pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w23Ki0phi0Ev",
        "outputId": "1de25895-e7de-41b1-ceec-918639766f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.13.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, LSTM, Bidirectional, Embedding, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Ensure compatible versions of numpy and gensim (if using gensim)\n",
        "# !pip install --upgrade numpy gensim\n",
        "\n",
        "# Load dataset\n",
        "data_path = '/content/NLP Manufacturer Dataset .csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Check dataset columns\n",
        "print(\"Columns in dataset:\", df.columns)\n",
        "\n",
        "# Define column names\n",
        "text_column = 'Input'  # Update if different\n",
        "label_column = 'Prediction'  # Update if different\n",
        "\n",
        "# Ensure columns exist\n",
        "assert text_column in df.columns, f\"Column '{text_column}' not found in dataset\"\n",
        "assert label_column in df.columns, f\"Column '{label_column}' not found in dataset\"\n",
        "\n",
        "# Drop NaN values\n",
        "df = df.dropna(subset=[text_column, label_column])\n",
        "\n",
        "# Ensure dataset is not empty after dropping NaN values\n",
        "if df.empty:\n",
        "    raise ValueError(\"Dataset is empty after dropping NaN values.\")\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df[label_column] = label_encoder.fit_transform(df[label_column])\n",
        "num_classes = df[label_column].nunique()\n",
        "\n",
        "# Adjust model output layer based on classification type\n",
        "if num_classes > 2:\n",
        "    activation = 'softmax'\n",
        "    loss_function = 'categorical_crossentropy'\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    y = to_categorical(df[label_column], num_classes=num_classes)\n",
        "else:\n",
        "    activation = 'sigmoid'\n",
        "    loss_function = 'binary_crossentropy'\n",
        "    y = df[label_column].values\n",
        "\n",
        "# Tokenization and padding\n",
        "max_words = 5000  # Vocabulary size\n",
        "max_length = 100  # Max sequence length\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df[text_column])\n",
        "X = tokenizer.texts_to_sequences(df[text_column])\n",
        "X = pad_sequences(X, maxlen=max_length)\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define CNN model\n",
        "def cnn_model(input_dim, input_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim, 128, input_length=input_length))\n",
        "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))  # FIXED pooling size\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation=activation))  # FIXED output layer\n",
        "    model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define LSTM model\n",
        "def lstm_model(input_dim, input_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim, 128, input_length=input_length))\n",
        "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(num_classes, activation=activation))\n",
        "    model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define CNN-BiLSTM model\n",
        "def cnn_bilstm_model(input_dim, input_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim, 128, input_length=input_length))\n",
        "    model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))  # FIXED pooling size\n",
        "    model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
        "    model.add(Dense(num_classes, activation=activation))\n",
        "    model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train CNN model\n",
        "cnn = cnn_model(max_words, max_length)\n",
        "cnn.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Train LSTM model\n",
        "lstm = lstm_model(max_words, max_length)\n",
        "lstm.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Train CNN-BiLSTM model\n",
        "cnn_bilstm = cnn_bilstm_model(max_words, max_length)\n",
        "cnn_bilstm.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate models\n",
        "cnn_score = cnn.evaluate(X_val, y_val)\n",
        "lstm_score = lstm.evaluate(X_val, y_val)\n",
        "cnn_bilstm_score = cnn_bilstm.evaluate(X_val, y_val)\n",
        "\n",
        "print(f\"CNN Accuracy: {cnn_score[1]*100:.2f}%\")\n",
        "print(f\"LSTM Accuracy: {lstm_score[1]*100:.2f}%\")\n",
        "print(f\"CNN-BiLSTM Accuracy: {cnn_bilstm_score[1]*100:.2f}%\")\n",
        "\n",
        "print(\"All models trained successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L69qDZwsfxH8",
        "outputId": "2aa89a9d-1609-474a-e84e-219cc66f78fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in dataset: Index(['Input', 'Prediction'], dtype='object')\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-b84ccdf151f2>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[label_column] = label_encoder.fit_transform(df[label_column])\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.4192 - loss: 1.0678 - val_accuracy: 0.8097 - val_loss: 0.7279\n",
            "Epoch 2/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.8945 - loss: 0.5001 - val_accuracy: 0.8226 - val_loss: 0.4954\n",
            "Epoch 3/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9409 - loss: 0.1885 - val_accuracy: 0.8516 - val_loss: 0.4225\n",
            "Epoch 4/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9865 - loss: 0.0660 - val_accuracy: 0.8323 - val_loss: 0.5109\n",
            "Epoch 5/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9980 - loss: 0.0212 - val_accuracy: 0.8355 - val_loss: 0.6052\n",
            "Epoch 6/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9987 - loss: 0.0102 - val_accuracy: 0.8323 - val_loss: 0.6056\n",
            "Epoch 1/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 158ms/step - accuracy: 0.4954 - loss: 1.0168 - val_accuracy: 0.8065 - val_loss: 0.6040\n",
            "Epoch 2/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 140ms/step - accuracy: 0.8767 - loss: 0.4259 - val_accuracy: 0.8387 - val_loss: 0.4996\n",
            "Epoch 3/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 133ms/step - accuracy: 0.9010 - loss: 0.2915 - val_accuracy: 0.8355 - val_loss: 0.4972\n",
            "Epoch 4/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step - accuracy: 0.9481 - loss: 0.1774 - val_accuracy: 0.8452 - val_loss: 0.5348\n",
            "Epoch 5/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 131ms/step - accuracy: 0.9602 - loss: 0.1218 - val_accuracy: 0.8484 - val_loss: 0.6054\n",
            "Epoch 6/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 133ms/step - accuracy: 0.9805 - loss: 0.0795 - val_accuracy: 0.8290 - val_loss: 0.6087\n",
            "Epoch 1/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 183ms/step - accuracy: 0.4237 - loss: 1.0428 - val_accuracy: 0.7935 - val_loss: 0.6218\n",
            "Epoch 2/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 160ms/step - accuracy: 0.8663 - loss: 0.4877 - val_accuracy: 0.8581 - val_loss: 0.4725\n",
            "Epoch 3/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 161ms/step - accuracy: 0.9082 - loss: 0.2751 - val_accuracy: 0.8484 - val_loss: 0.4551\n",
            "Epoch 4/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 148ms/step - accuracy: 0.9516 - loss: 0.1523 - val_accuracy: 0.8258 - val_loss: 0.5601\n",
            "Epoch 5/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 150ms/step - accuracy: 0.9872 - loss: 0.0516 - val_accuracy: 0.8484 - val_loss: 0.6508\n",
            "Epoch 6/10\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 152ms/step - accuracy: 0.9954 - loss: 0.0265 - val_accuracy: 0.8258 - val_loss: 0.7632\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8296 - loss: 0.4405 \n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8152 - loss: 0.5302\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8195 - loss: 0.4862\n",
            "CNN Accuracy: 85.16%\n",
            "LSTM Accuracy: 83.55%\n",
            "CNN-BiLSTM Accuracy: 84.84%\n",
            "All models trained successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform labels\n",
        "test_data[\"Prediction\"] = label_encoder.fit_transform(test_data[\"Prediction\"])\n",
        "\n",
        "# Get the true labels\n",
        "y_true = test_data[\"Prediction\"]\n",
        "\n",
        "# Check if encoding was successful\n",
        "print(test_data[\"Prediction\"].unique())  # This should print numeric values like [0, 1, 2] instead of text\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kEQDS4YQoeEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "221aa313-463d-45e6-96bb-805d8d4db95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "\n",
        "# Ensure correct shape for input sequences\n",
        "max_length = 100  # Match training setup\n",
        "test_data[\"Input\"] = test_data[\"Input\"].astype(str)\n",
        "\n",
        "# Convert text input to sequences\n",
        "X_test_seq = tokenizer.texts_to_sequences(test_data[\"Input\"])\n",
        "X_test_seq = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Verify shape\n",
        "print(\"Shape of X_test_seq:\", X_test_seq.shape)  # Should be (num_samples, max_length)\n",
        "\n",
        "# Encode actual labels correctly\n",
        "label_encoder = LabelEncoder()\n",
        "test_data[\"Prediction\"] = label_encoder.fit_transform(test_data[\"Prediction\"])  # Convert text labels to numbers\n",
        "y_true = test_data[\"Prediction\"].astype(int)  # Ensure it's numeric and 1D\n",
        "\n",
        "# Ensure models are trained before prediction\n",
        "try:\n",
        "    y_pred_cnn = cnn.predict(X_test_seq)\n",
        "    y_pred_lstm = lstm.predict(X_test_seq)\n",
        "    y_pred_cnn_bilstm = cnn_bilstm.predict(X_test_seq)\n",
        "\n",
        "    # Convert predictions to match `y_true`\n",
        "    if y_pred_cnn.shape[1] > 1:\n",
        "        y_pred_cnn = np.argmax(y_pred_cnn, axis=1)  # Multiclass case\n",
        "        y_pred_lstm = np.argmax(y_pred_lstm, axis=1)\n",
        "        y_pred_cnn_bilstm = np.argmax(y_pred_cnn_bilstm, axis=1)\n",
        "    else:\n",
        "        y_pred_cnn = (y_pred_cnn > 0.5).astype(int).ravel()  # Binary classification case\n",
        "        y_pred_lstm = (y_pred_lstm > 0.5).astype(int).ravel()\n",
        "        y_pred_cnn_bilstm = (y_pred_cnn_bilstm > 0.5).astype(int).ravel()\n",
        "\n",
        "    # Check final shapes\n",
        "    print(\"Shape of y_true:\", y_true.shape)\n",
        "    print(\"Shape of y_pred_cnn:\", y_pred_cnn.shape)\n",
        "\n",
        "    # Evaluate models\n",
        "    print(\"CNN Accuracy:\", accuracy_score(y_true, y_pred_cnn))\n",
        "    print(\"CNN Classification Report:\\n\", classification_report(y_true, y_pred_cnn))\n",
        "\n",
        "    print(\"LSTM Accuracy:\", accuracy_score(y_true, y_pred_lstm))\n",
        "    print(\"LSTM Classification Report:\\n\", classification_report(y_true, y_pred_lstm))\n",
        "\n",
        "    print(\"CNN-BiLSTM Accuracy:\", accuracy_score(y_true, y_pred_cnn_bilstm))\n",
        "    print(\"CNN-BiLSTM Classification Report:\\n\", classification_report(y_true, y_pred_cnn_bilstm))\n",
        "\n",
        "except NameError as e:\n",
        "    print(\"Error: Model not found. Ensure CNN, LSTM, and CNN-BiLSTM models are trained before prediction.\")\n",
        "    print(\"Detailed Error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-snfVSKKkr3R",
        "outputId": "13178e71-8193-4772-cb4e-c9bd97e4c63b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_test_seq: (155, 100)\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ee030715760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ee030715760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 194ms/step\n",
            "Shape of y_true: (155,)\n",
            "Shape of y_pred_cnn: (155,)\n",
            "CNN Accuracy: 0.3032258064516129\n",
            "CNN Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        42\n",
            "           1       0.30      1.00      0.47        47\n",
            "           2       0.00      0.00      0.00        66\n",
            "\n",
            "    accuracy                           0.30       155\n",
            "   macro avg       0.10      0.33      0.16       155\n",
            "weighted avg       0.09      0.30      0.14       155\n",
            "\n",
            "LSTM Accuracy: 0.3032258064516129\n",
            "LSTM Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        42\n",
            "           1       0.30      1.00      0.47        47\n",
            "           2       0.00      0.00      0.00        66\n",
            "\n",
            "    accuracy                           0.30       155\n",
            "   macro avg       0.10      0.33      0.16       155\n",
            "weighted avg       0.09      0.30      0.14       155\n",
            "\n",
            "CNN-BiLSTM Accuracy: 0.3032258064516129\n",
            "CNN-BiLSTM Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        42\n",
            "           1       0.30      1.00      0.47        47\n",
            "           2       0.00      0.00      0.00        66\n",
            "\n",
            "    accuracy                           0.30       155\n",
            "   macro avg       0.10      0.33      0.16       155\n",
            "weighted avg       0.09      0.30      0.14       155\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the best model (assuming CNN performed best)\n",
        "cnn.save('manufacturer_classification_model.h5')\n",
        "print(\"Best model saved as 'manufacturer_classification_model.h5'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUdnQswTuLIL",
        "outputId": "008798a7-c03d-4bba-abe5-434dedb32347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved as 'manufacturer_classification_model.h5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "csb5SbOL1Z-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model before making predictions\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('manufacturer_classification_model.h5')\n",
        "\n",
        "# Tokenize the input text\n",
        "new_text = \"This is a sample input text for prediction.\"  # Define the new_text variable\n",
        "sequence = tokenizer.texts_to_sequences([new_text])\n",
        "\n",
        "# Ensure the sequence has a minimum length (should match training input length)\n",
        "padded_sequence = pad_sequences(sequence, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "# Convert to NumPy array\n",
        "input_data = np.array(padded_sequence)\n",
        "\n",
        "# Predict\n",
        "predicted_probabilities = model.predict(input_data)\n",
        "\n",
        "print(\"Input shape before prediction:\", input_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26vwcW-c7X_Y",
        "outputId": "6092d453-b2fe-4a98-e909-5081a4ef0060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "Input shape before prediction: (1, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_class(new_text):\n",
        "    # Convert text to sequence using the same tokenizer from training\n",
        "    sequence = tokenizer.texts_to_sequences([new_text])\n",
        "\n",
        "    # If empty sequence, add a dummy placeholder\n",
        "    if len(sequence[0]) == 0:\n",
        "        sequence = [[1] * 10]  # Adjust length if needed\n",
        "\n",
        "    # Pad sequence to the required length\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "    # Convert to NumPy array\n",
        "    input_data = np.array(padded_sequence)\n",
        "\n",
        "    # Debug: Check input shape before prediction\n",
        "    print(\"Input shape before prediction:\", input_data.shape)\n",
        "\n",
        "    # Make prediction\n",
        "    predicted_probabilities = model.predict(input_data)\n",
        "\n",
        "    # Get class with highest probability\n",
        "    predicted_class = np.argmax(predicted_probabilities, axis=1)[0]\n",
        "    predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
        "\n",
        "    confidence_score = np.max(predicted_probabilities)\n",
        "    return predicted_label, confidence_score"
      ],
      "metadata": {
        "id": "DxjAIqa96qaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the trained LabelEncoder\n",
        "with open('label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"Label Encoder saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28HpvfFPZg_4",
        "outputId": "43ab76e2-19d5-4d1b-e25b-397428ae024a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Encoder saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the trained model and preprocessing objects\n",
        "model = load_model('manufacturer_classification_model.h5')\n",
        "bow_vectorizer = joblib.load('bow_vectorizer.pkl')\n",
        "label_encoder = joblib.load('label_encoder.pkl')\n",
        "\n",
        "# Define class labels\n",
        "class_labels = {\n",
        "    0: \"Compliant Product\",\n",
        "    1: \"Minor Defect\",\n",
        "    2: \"Major Issue\"\n",
        "}\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabet characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Function to predict class\n",
        "def predict_class(new_text):\n",
        "    processed_text = preprocess_text(new_text)\n",
        "    input_data = bow_vectorizer.transform([processed_text]).toarray()\n",
        "\n",
        "    # Ensure input shape matches the model's expected input\n",
        "    expected_shape = model.input_shape[1]  # CNN expects fixed-length input\n",
        "\n",
        "    if input_data.shape[1] < expected_shape:\n",
        "        # If input is too short, pad it with zeros\n",
        "        padding = np.zeros((1, expected_shape - input_data.shape[1]))\n",
        "        input_data = np.hstack((input_data, padding))\n",
        "    elif input_data.shape[1] > expected_shape:\n",
        "        # If input is too long, truncate it\n",
        "        input_data = input_data[:, :expected_shape]\n",
        "\n",
        "    # Predict\n",
        "    predicted_probabilities = model.predict(input_data)\n",
        "    predicted_class = np.argmax(predicted_probabilities, axis=1)[0]\n",
        "    confidence_score = np.max(predicted_probabilities)\n",
        "\n",
        "    # Get the corresponding label\n",
        "    predicted_label = class_labels.get(predicted_class, \"Unknown Class\")\n",
        "\n",
        "    return predicted_class, predicted_label, confidence_score\n",
        "\n",
        "# Taking user input\n",
        "new_text = input(\"Enter your sample text: \").strip()\n",
        "predicted_class, predicted_label, confidence_score = predict_class(new_text)\n",
        "\n",
        "# Display results\n",
        "print(f\"\\nProcessed Text: {preprocess_text(new_text)}\")\n",
        "print(f\"Predicted Class: {predicted_class} - {predicted_label}\")\n",
        "print(f\"Confidence Score: {confidence_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy235gomu8_o",
        "outputId": "e0bd0b14-404c-46cf-b9b7-4daffefc92a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your sample text: Some rubber seals showed small air bubbles from the molding process. These bubbles slightly affected sealing efficiency but did not cause significant leaks. A more refined molding technique could eliminate this issue. The seals were still usable with minor improvements.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\n",
            "Processed Text: some rubber seals showed small air bubbles from the molding process these bubbles slightly affected sealing efficiency but did not cause significant leaks a more refined molding technique could eliminate this issue the seals were still usable with minor improvements\n",
            "Predicted Class: 1 - Minor Defect\n",
            "Confidence Score: 0.8386\n"
          ]
        }
      ]
    }
  ]
}